{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"hw7_bert","provenance":[{"file_id":"https://github.com/ga642381/ML2021-Spring/blob/main/HW07/HW07.ipynb","timestamp":1620559970324}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8142bb0952e14a84be0d71da4172fbe9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_60d107af77554f6dac4d974967d594b8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e34ed58275a449ad88acebddd9ea0a8d","IPY_MODEL_92a7c39016d2463ea715117d3810d092"]}},"60d107af77554f6dac4d974967d594b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e34ed58275a449ad88acebddd9ea0a8d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_da33ef52858c435b8001e39286b7f2f3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":660,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":660,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6cfb898edc7a45d4b9489aaac60f4278"}},"92a7c39016d2463ea715117d3810d092":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_db64274252164bbe8a535db043fb0316","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 660/660 [00:52&lt;00:00, 12.7B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b3a1fe09c24243b1baa612de2601211b"}},"da33ef52858c435b8001e39286b7f2f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6cfb898edc7a45d4b9489aaac60f4278":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db64274252164bbe8a535db043fb0316":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b3a1fe09c24243b1baa612de2601211b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01c60436816f471ea785cfd20983eb9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e0364c8735034b238524922041bbdd1c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3ad28c013e1b44388a1c4f2385facfb8","IPY_MODEL_7fb5eb6e734e4772b4edfd34c08f490e"]}},"e0364c8735034b238524922041bbdd1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ad28c013e1b44388a1c4f2385facfb8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_01b5b60f9cf04833ad6934b8828918ad","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1306488754,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1306488754,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1a9a7f31efa341a8829f295b05855470"}},"7fb5eb6e734e4772b4edfd34c08f490e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2bf0b8c283c54862a7a4dd04a2e8312b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.31G/1.31G [08:41&lt;00:00, 2.51MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b552102008634d768aade350d9ea4350"}},"01b5b60f9cf04833ad6934b8828918ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1a9a7f31efa341a8829f295b05855470":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2bf0b8c283c54862a7a4dd04a2e8312b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b552102008634d768aade350d9ea4350":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e3cef61efa24db9868b6661cd2e10c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d3300ec1adea4e2aa05aeddb908d78b1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_72ed3fb3f59e48e6b88d1fb9be313e49","IPY_MODEL_91c4577558a14c5788eba800181d7b9a"]}},"d3300ec1adea4e2aa05aeddb908d78b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72ed3fb3f59e48e6b88d1fb9be313e49":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_333c5343ce7f4f0a82fb583068351193","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":109540,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":109540,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d9460fba7a7447de8841f69b9f264d8d"}},"91c4577558a14c5788eba800181d7b9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_73b89ad4a6c4452dbb857443f40580ab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 110k/110k [00:01&lt;00:00, 95.2kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f67232d38aa545e09e70d50f17d97886"}},"333c5343ce7f4f0a82fb583068351193":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d9460fba7a7447de8841f69b9f264d8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73b89ad4a6c4452dbb857443f40580ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f67232d38aa545e09e70d50f17d97886":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec2745a229be4054b4abb7f5982fd284":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4f33f8f1985b4fad8afb048c0453ddfd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3038413ef39c42b2b5a1532910eff529","IPY_MODEL_dd5a636091fa4190b87f67ca59a4aa3b"]}},"4f33f8f1985b4fad8afb048c0453ddfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3038413ef39c42b2b5a1532910eff529":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c47836f9d94b4e96b1aceb39ea5db742","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":268961,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":268961,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cdaf4f81ca0c48949a95dcd019e9568f"}},"dd5a636091fa4190b87f67ca59a4aa3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_13a69e23b27c4464a99ac0d5c96a0169","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 269k/269k [00:00&lt;00:00, 467kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_531ac18f8cde4003a2c48dd039727987"}},"c47836f9d94b4e96b1aceb39ea5db742":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cdaf4f81ca0c48949a95dcd019e9568f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"13a69e23b27c4464a99ac0d5c96a0169":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"531ac18f8cde4003a2c48dd039727987":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c9d87e2829644f5e82f13c4f82dd48ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bca76532bc294b1c8c153c66bafdc2bf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b9e5620bdaee4fb7ad9f26bd0ab0143a","IPY_MODEL_82d430bbd8994885bdccd85800b734fa"]}},"bca76532bc294b1c8c153c66bafdc2bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b9e5620bdaee4fb7ad9f26bd0ab0143a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_44b68221049145fca3e952b64df4f5db","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_96e072f54093451d92ac7a13e0c13ad2"}},"82d430bbd8994885bdccd85800b734fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d9c37ae9dd9b498d9d8f0882ec949862","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.00/2.00 [02:42&lt;00:00, 81.3s/B]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c871b79d59544b72ba61743d787cfa4b"}},"44b68221049145fca3e952b64df4f5db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"96e072f54093451d92ac7a13e0c13ad2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d9c37ae9dd9b498d9d8f0882ec949862":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c871b79d59544b72ba61743d787cfa4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1f494de56a924df49771ab815b0d8335":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c6256c3370d0422fa22b49b0af2c2976","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6ad0c15908764f679223ddb884ddc80e","IPY_MODEL_6a7f888f1f9d44bd83f176c05b9621ae"]}},"c6256c3370d0422fa22b49b0af2c2976":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6ad0c15908764f679223ddb884ddc80e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b4e9305f15ac489caf517645ee3b685e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a5d7b475309b4b0ea5bdb7a05f2ce021"}},"6a7f888f1f9d44bd83f176c05b9621ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_85b983a510624c78bee4aa045261d618","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [02:42&lt;00:00, 1.45s/B]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ffdef32f5943444fb9341b2f274562e6"}},"b4e9305f15ac489caf517645ee3b685e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a5d7b475309b4b0ea5bdb7a05f2ce021":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85b983a510624c78bee4aa045261d618":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ffdef32f5943444fb9341b2f274562e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"703e289b9d484883a01b364fcda44227":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b755bd010a9640b6983f05ad91e7df73","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fed708fc37bf4c02acde27385a575778","IPY_MODEL_13f615391c28450f834d7dfa5dbb2e70"]}},"b755bd010a9640b6983f05ad91e7df73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fed708fc37bf4c02acde27385a575778":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_39b60b46962a4d0b9528ecfd36ef6b2d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":19,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":19,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6b2ff6e51387403c83263881e2454773"}},"13f615391c28450f834d7dfa5dbb2e70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_03da0c90925644ad972e9c2014dae41f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 19.0/19.0 [02:41&lt;00:00, 8.52s/B]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ab62208464047c7b7134f4ee4f9756a"}},"39b60b46962a4d0b9528ecfd36ef6b2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6b2ff6e51387403c83263881e2454773":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"03da0c90925644ad972e9c2014dae41f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0ab62208464047c7b7134f4ee4f9756a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5cc0f2a75a72493eb4957b8f61c2993c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ef0db578f8414ec381ff35cc2ee8b0c1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d988337970e84a42b0286ebaae517dc5","IPY_MODEL_85d15fe0d7b74bdca99960a4f93d9ba9"]}},"ef0db578f8414ec381ff35cc2ee8b0c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d988337970e84a42b0286ebaae517dc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ceee4c67fc8d49909faa8d6364a18c92","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"danger","max":26936,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":47,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b13c8958e7a14679aaa036967ec5e0e9"}},"85d15fe0d7b74bdca99960a4f93d9ba9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e2385d7872954bc6913c67c3f5894aab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 47/26936 [00:09&lt;1:31:38,  4.89it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6807ba06bd7441fe862996ae7c7530e4"}},"ceee4c67fc8d49909faa8d6364a18c92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b13c8958e7a14679aaa036967ec5e0e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2385d7872954bc6913c67c3f5894aab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6807ba06bd7441fe862996ae7c7530e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"xvSGDbExff_I"},"source":["# **Homework 7 - Bert (Question Answering)**\n","\n","If you have any questions, feel free to email us at ntu-ml-2021spring-ta@googlegroups.com\n","\n","\n","\n","Slide:    [Link](https://docs.google.com/presentation/d/1aQoWogAQo_xVJvMQMrGaYiWzuyfO0QyLLAhiMwFyS2w)　Kaggle: [Link](https://www.kaggle.com/c/ml2021-spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WGOr_eS3wJJf"},"source":["## Task description\n","- Chinese Extractive Question Answering\n","  - Input: Paragraph + Question\n","  - Output: Answer\n","\n","- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n","\n","- Todo\n","    - Fine tune a pretrained chinese BERT model\n","    - Change hyperparameters (e.g. doc_stride)\n","    - Apply linear learning rate decay\n","    - Try other pretrained models\n","    - Improve preprocessing\n","    - Improve postprocessing\n","- Training tips\n","    - Automatic mixed precision\n","    - Gradient accumulation\n","    - Ensemble\n","\n","- Estimated training time (tesla t4 with automatic mixed precision enabled)\n","    - Simple: 8mins\n","    - Medium: 8mins\n","    - Strong: 25mins\n","    - Boss: 2hrs\n","  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsUQ9uxv7k54","executionInfo":{"status":"ok","timestamp":1621664130096,"user_tz":-480,"elapsed":1860,"user":{"displayName":"李亭臻","photoUrl":"","userId":"05946528128840806794"}},"outputId":"3709f2a4-d5c4-40c1-d7a6-df54d80776a9"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sat May 22 06:15:29 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bi4fKgBP8DRt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621664155108,"user_tz":-480,"elapsed":23234,"user":{"displayName":"李亭臻","photoUrl":"","userId":"05946528128840806794"}},"outputId":"54d4d4bb-efa3-41a0-bfbc-e7d56b8306d3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ehSlJ_V38RJ_"},"source":["workspace_dir = '/content/drive/MyDrive/ML2021-hw7'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJ1fSAJE2oaC"},"source":["## Download Dataset"]},{"cell_type":"code","metadata":{"id":"YPrc4Eie9Yo5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621664156263,"user_tz":-480,"elapsed":4376,"user":{"displayName":"李亭臻","photoUrl":"","userId":"05946528128840806794"}},"outputId":"7f7ad64d-fdec-4129-92b7-6d84e3272176"},"source":["# Download link 1\n","# !gdown --id '1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1' --output hw7_data.zip --output \"{workspace_dir}/hw7_data.zip\"\n","\n","# Download Link 2 (if the above link fails) \n","# !gdown --id '1pOu3FdPdvzielUZyggeD7KDnVy9iW1uC' --output hw7_data.zip\n","\n","!unzip -o \"{workspace_dir}/hw7_data.zip\" -d \"./\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/MyDrive/ML2021-hw7/hw7_data.zip\n","  inflating: ./hw7_dev.json          \n","  inflating: ./hw7_test.json         \n","  inflating: ./hw7_train.json        \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TevOvhC03m0h"},"source":["## Install transformers\n","\n","Documentation for the toolkit:　https://huggingface.co/transformers/"]},{"cell_type":"code","metadata":{"id":"tbxWFX_jpDom","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621664681441,"user_tz":-480,"elapsed":5816,"user":{"displayName":"李亭臻","photoUrl":"","userId":"05946528128840806794"}},"outputId":"87b5643c-28ca-4cbc-f2a4-3c8636ac6889"},"source":["# You are allowed to change version of transformers or use other toolkits\n","!pip install transformers==4.5.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers==4.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.2MB 6.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 17.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.0.12)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.0.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 53.2MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (8.0.0)\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8dKM4yCh4LI_"},"source":["## Import Packages"]},{"cell_type":"code","metadata":{"id":"WOTHHtWJoahe"},"source":["import json\n","import numpy as np\n","import random\n","import torch\n","from torch.utils.data import DataLoader, Dataset \n","from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast\n","import random\n","from tqdm.auto import tqdm\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Fix random seed for reproducibility\n","def same_seeds(seed):\n","\t  torch.manual_seed(seed)\n","\t  if torch.cuda.is_available():\n","\t\t    torch.cuda.manual_seed(seed)\n","\t\t    torch.cuda.manual_seed_all(seed)\n","\t  np.random.seed(seed)\n","\t  random.seed(seed)\n","\t  torch.backends.cudnn.benchmark = False\n","\t  torch.backends.cudnn.deterministic = True\n","same_seeds(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7pBtSZP1SKQO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621659256302,"user_tz":-480,"elapsed":36226,"user":{"displayName":"李亭臻","photoUrl":"","userId":"05946528128840806794"}},"outputId":"24666d5b-3a5f-4b12-cf68-d334b84186b8"},"source":["# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\t\n","fp16_training = True\n","\n","if fp16_training:\n","    !pip install accelerate==0.2.0\n","    from accelerate import Accelerator\n","    accelerator = Accelerator(fp16=True)\n","    device = accelerator.device\n","\n","# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting accelerate==0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/c6/6f08def78c19e328335236ec283a7c70e73913d1ed6f653ce2101bfad139/accelerate-0.2.0-py3-none-any.whl (47kB)\n","\r\u001b[K     |███████                         | 10kB 24.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 20kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 30kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 40kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.1MB/s \n","\u001b[?25hCollecting pyaml>=20.4.0\n","  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate==0.2.0) (1.8.1+cu101)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate==0.2.0) (3.13)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate==0.2.0) (3.7.4.3)\n","Installing collected packages: pyaml, accelerate\n","Successfully installed accelerate-0.2.0 pyaml-20.4.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2YgXHuVLp_6j"},"source":["## Load Model and Tokenizer\n","\n","\n","\n","\n"," "]},{"cell_type":"code","metadata":{"id":"xyBCYGjAp3ym","colab":{"base_uri":"https://localhost:8080/","height":479,"referenced_widgets":["8142bb0952e14a84be0d71da4172fbe9","60d107af77554f6dac4d974967d594b8","e34ed58275a449ad88acebddd9ea0a8d","92a7c39016d2463ea715117d3810d092","da33ef52858c435b8001e39286b7f2f3","6cfb898edc7a45d4b9489aaac60f4278","db64274252164bbe8a535db043fb0316","b3a1fe09c24243b1baa612de2601211b","01c60436816f471ea785cfd20983eb9d","e0364c8735034b238524922041bbdd1c","3ad28c013e1b44388a1c4f2385facfb8","7fb5eb6e734e4772b4edfd34c08f490e","01b5b60f9cf04833ad6934b8828918ad","1a9a7f31efa341a8829f295b05855470","2bf0b8c283c54862a7a4dd04a2e8312b","b552102008634d768aade350d9ea4350","5e3cef61efa24db9868b6661cd2e10c7","d3300ec1adea4e2aa05aeddb908d78b1","72ed3fb3f59e48e6b88d1fb9be313e49","91c4577558a14c5788eba800181d7b9a","333c5343ce7f4f0a82fb583068351193","d9460fba7a7447de8841f69b9f264d8d","73b89ad4a6c4452dbb857443f40580ab","f67232d38aa545e09e70d50f17d97886","ec2745a229be4054b4abb7f5982fd284","4f33f8f1985b4fad8afb048c0453ddfd","3038413ef39c42b2b5a1532910eff529","dd5a636091fa4190b87f67ca59a4aa3b","c47836f9d94b4e96b1aceb39ea5db742","cdaf4f81ca0c48949a95dcd019e9568f","13a69e23b27c4464a99ac0d5c96a0169","531ac18f8cde4003a2c48dd039727987","c9d87e2829644f5e82f13c4f82dd48ee","bca76532bc294b1c8c153c66bafdc2bf","b9e5620bdaee4fb7ad9f26bd0ab0143a","82d430bbd8994885bdccd85800b734fa","44b68221049145fca3e952b64df4f5db","96e072f54093451d92ac7a13e0c13ad2","d9c37ae9dd9b498d9d8f0882ec949862","c871b79d59544b72ba61743d787cfa4b","1f494de56a924df49771ab815b0d8335","c6256c3370d0422fa22b49b0af2c2976","6ad0c15908764f679223ddb884ddc80e","6a7f888f1f9d44bd83f176c05b9621ae","b4e9305f15ac489caf517645ee3b685e","a5d7b475309b4b0ea5bdb7a05f2ce021","85b983a510624c78bee4aa045261d618","ffdef32f5943444fb9341b2f274562e6","703e289b9d484883a01b364fcda44227","b755bd010a9640b6983f05ad91e7df73","fed708fc37bf4c02acde27385a575778","13f615391c28450f834d7dfa5dbb2e70","39b60b46962a4d0b9528ecfd36ef6b2d","6b2ff6e51387403c83263881e2454773","03da0c90925644ad972e9c2014dae41f","0ab62208464047c7b7134f4ee4f9756a"]},"executionInfo":{"status":"ok","timestamp":1621664765851,"user_tz":-480,"elapsed":76548,"user":{"displayName":"李亭臻","photoUrl":"","userId":"05946528128840806794"}},"outputId":"c46a051e-c2ad-4bf4-ac0a-a81d4608175d"},"source":["model = BertForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-large\").to(device)\n","tokenizer = BertTokenizerFast.from_pretrained(\"hfl/chinese-macbert-large\")\n","\n","# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8142bb0952e14a84be0d71da4172fbe9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=660.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01c60436816f471ea785cfd20983eb9d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1306488754.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at hfl/chinese-macbert-large were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e3cef61efa24db9868b6661cd2e10c7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=109540.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec2745a229be4054b4abb7f5982fd284","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=268961.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9d87e2829644f5e82f13c4f82dd48ee","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f494de56a924df49771ab815b0d8335","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"703e289b9d484883a01b364fcda44227","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=19.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3Td-GTmk5OW4"},"source":["## Read Data\n","\n","- Training set: 26935 QA pairs\n","- Dev set: 3523  QA pairs\n","- Test set: 3492  QA pairs\n","\n","- {train/dev/test}_questions:\t\n","  - List of dicts with the following keys:\n","   - id (int)\n","   - paragraph_id (int)\n","   - question_text (string)\n","   - answer_text (string)\n","   - answer_start (int)\n","   - answer_end (int)\n","- {train/dev/test}_paragraphs: \n","  - List of strings\n","  - paragraph_ids in questions correspond to indexs in paragraphs\n","  - A paragraph may be used by several questions "]},{"cell_type":"code","metadata":{"id":"NvX7hlepogvu"},"source":["def read_data(file):\n","    with open(file, 'r', encoding=\"utf-8\") as reader:\n","        data = json.load(reader)\n","    return data[\"questions\"], data[\"paragraphs\"]\n","\n","train_questions, train_paragraphs = read_data(\"hw7_train.json\")\n","dev_questions, dev_paragraphs = read_data(\"hw7_dev.json\")\n","test_questions, test_paragraphs = read_data(\"hw7_test.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fm0rpTHq0e4N"},"source":["## Tokenize Data"]},{"cell_type":"code","metadata":{"id":"rTZ6B70Hoxie"},"source":["# Tokenize questions and paragraphs separately\n","# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n","\n","train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n","dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n","test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n","\n","train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n","dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n","test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n","\n","# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ws8c8_4d5UCI"},"source":["## Dataset and Dataloader"]},{"cell_type":"code","metadata":{"id":"Xjooag-Swnuh"},"source":["class QA_Dataset(Dataset):\n","    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n","        self.split = split\n","        self.questions = questions\n","        self.tokenized_questions = tokenized_questions\n","        self.tokenized_paragraphs = tokenized_paragraphs\n","        self.max_question_len = 40\n","        self.max_paragraph_len = 150\n","        \n","        ##### TODO: Change value of doc_stride #####\n","        self.doc_stride = 5\n","\n","        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n","        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        question = self.questions[idx]\n","        tokenized_question = self.tokenized_questions[idx]\n","        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n","\n","        ##### TODO: Preprocessing #####\n","        # Hint: How to prevent model from learning something it should not learn\n","\n","        if self.split == \"train\":\n","            # input_ids_list, token_type_ids_list, attention_mask_list, answer_start_token_list, answer_end_token_list = [], [], [], [], []\n","\n","            \n","            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n","            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n","            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n","\n","            # for i in range(answer_end_token - self.max_paragraph_len, answer_start_token, self.doc_stride):\n","                \n","            #     # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n","            #     input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n","            #     input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n","\n","            #     answer_start = answer_start_token + len(input_ids_question) - i\n","            #     answer_end = answer_end_token + len(input_ids_question) - i\n","                \n","            #     # Pad sequence and obtain inputs to model\n","            #     input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n","                \n","            #     input_ids_list.append(input_ids)\n","            #     token_type_ids_list.append(token_type_ids)\n","            #     attention_mask_list.append(attention_mask)\n","            #     answer_start_token_list.append(answer_start)\n","            #     answer_end_token_list.append(answer_end)\n","\n","\n","            rand_int = random.randint(2, 7)\n","            # A single window is obtained by slicing the portion of paragraph containing the answer\n","            mid = (answer_start_token + answer_end_token) // 2\n","            paragraph_start = max(0, min(mid - self.max_paragraph_len // rand_int , len(tokenized_paragraph) - self.max_paragraph_len))\n","            paragraph_end = paragraph_start + self.max_paragraph_len\n","            \n","            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n","            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n","            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n","            \n","            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n","            answer_start_token += len(input_ids_question) - paragraph_start\n","            answer_end_token += len(input_ids_question) - paragraph_start\n","            \n","            Pad sequence and obtain inputs to model \n","            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n","            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n","            # return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list), torch.tensor(answer_start_token_list), torch.tensor(answer_end_token_list)\n","\n","        # Validation/Testing\n","        else:\n","            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n","            \n","            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n","            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n","                \n","                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n","                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n","                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n","                \n","                # Pad sequence and obtain inputs to model\n","                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n","                \n","                input_ids_list.append(input_ids)\n","                token_type_ids_list.append(token_type_ids)\n","                attention_mask_list.append(attention_mask)\n","            \n","            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n","\n","    def padding(self, input_ids_question, input_ids_paragraph):\n","        # Pad zeros if sequence length is shorter than max_seq_len\n","        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n","        # Indices of input sequence tokens in the vocabulary\n","        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n","        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n","        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n","        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n","        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n","        \n","        return input_ids, token_type_ids, attention_mask\n","\n","train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n","dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n","test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n","\n","train_batch_size = 16\n","\n","# Note: Do NOT change batch size of dev_loader / test_loader !\n","# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n","train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n","dev_loader = DataLoader(dev_set, batch_size=1, shuffle=True, pin_memory=True)\n","test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_tVuKEFl6mQ","colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"status":"ok","timestamp":1621659327440,"user_tz":-480,"elapsed":107329,"user":{"displayName":"李亭臻","photoUrl":"","userId":"05946528128840806794"}},"outputId":"4c8b8fdb-b9b8-4e9c-ddc0-aa3777e40291"},"source":["test_paragraphs[792]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'一般隊員及副下士穿著「三色」制服，且沒有佩掛任何軍階，外表除了制服大小外並無不同。至於下士的制服，在袖口的部分，有紅色的編織徽章，且持有的戟更像茅的樣子。頭部的部分，通常是黑色貝雷帽，或是帶有紅色、白色、黃色和黑色以及紫色鴕鳥羽毛的黑色無面高頂盔上，前者主要在擔任守衛職務或演習時配戴，後者則在禮儀儀式，例如宣誓就職儀式或接待外國元首、外賓時配戴。而傳統上，羽毛是使用亮豔環頸雉或是蒼鷺的羽毛。而高階的軍官們，則有不同形式的制服。所有士官的制服基本上與士兵是一樣的，除了黑色長袍和紅色馬褲。士官的頭盔上都會有一根紅色羽毛，但士官長的頭盔則是白色羽毛，以顯示獨特性。當瑞士近衛隊著儀式制服時，士官會有不同的枚徽，且胸前會有金線穿越。高階軍官的制服會用全深紅色的制服、袖口的金色刺繡、以及完全不同樣式的馬褲來和士兵及士官的制服做區隔，而他們也佩有一隻長劍可以用來指揮小隊或中隊。在著儀式制服時，除指揮官的頭盔配白色羽毛外，其他軍士官兵頭盔皆配紫色羽毛。而在出席儀式時，指揮官和參謀長會著盔甲。而在這樣的場合，所謂的「完整盔甲」，是包含袖部都必須穿著盔甲。除典禮場合外，一般守衛時會著值勤制服。'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"0cMMuRHNknKx","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1621659327441,"user_tz":-480,"elapsed":107322,"user":{"displayName":"李亭臻","photoUrl":"","userId":"05946528128840806794"}},"outputId":"2eafca8c-5a48-47ab-92f5-27dbd18f6e32"},"source":["test_paragraphs[0][1:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'探討'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"5_H1kqhR8CdM"},"source":["## Function for Evaluation"]},{"cell_type":"code","metadata":{"id":"SqeA3PLPxOHu"},"source":["def evaluate(data, output, count, Test=False):\n","    ##### TODO: Postprocessing #####\n","    # There is a bug and room for improvement in postprocessing \n","    # Hint: Open your prediction file to see what is wrong \n","    \n","    answer = ''\n","    unk = '[UNK]'\n","    max_prob = float('-inf')\n","    num_of_windows = data[0].shape[1]\n","    paragraph_id = test_questions[count]['paragraph_id']\n","    question_len = len(test_questions_tokenized[count]) + 2\n","    # print(\"paragraph_id\", paragraph_id)\n","    for k in range(num_of_windows):\n","        # Obtain answer by choosing the most probable start position / end position\n","        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n","        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n","        \n","        \n","        # Probability of answer is calculated as sum of start_prob and end_prob\n","        prob = start_prob + end_prob\n","        \n","        # Replace answer if calculated probability is larger than previous windows\n","        if end_index > start_index and  prob > max_prob and start_index >= question_len:\n","            max_prob = prob\n","            if Test:\n","                print(\"window_len:\", len(output.start_logits[k]))\n","                print(\"window:\", k)\n","                print(\"question_len:\", question_len)\n","                print(\"start and end index:\", start_index, end_index)\n","                # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n","                p_start_index = start_index  + (5*k) - (question_len) \n","                p_end_index = end_index + (5*k) - (question_len) \n","                print(\"p_start_index:\", p_start_index)\n","                # start_range = test_paragraphs_tokenized[paragraph_id].token_to_chars(p_start_index)\n","                # if p_start_index < len(test_paragraphs_tokenized[paragraph_id]):\n","                #     if p_start_index != 0:\n","                #         start = test_paragraphs_tokenized[paragraph_id].token_to_chars(p_start_index)[0]\n","                #     else:\n","                #         start = 0\n","                #     if end_index > 193-question_len+1:\n","                #         continue\n","                #     else:\n","                #         end = test_paragraphs_tokenized[paragraph_id].token_to_chars(p_end_index)[1]\n","                #     answer = test_paragraphs[paragraph_id][start : end]\n","\n","                answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n","                # print(\"start:\", start_range)\n","                \n","                # print(\"start and end :\", start, end)\n","                print(\"answer:\", answer)\n","            else:\n","                answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n","                \n","                ## Deal with English lowercase \n","                contained_english = False\n","                chars = answer.split()\n","                for item in chars:\n","                    if item.encode().isalpha():\n","                        contained_english = True\n","\n","                \n","                if unk in answer or contained_english:\n","                    p_start_index = start_index  + (5*k) - (question_len) \n","                    p_end_index = end_index + (5*k) - (question_len) \n","                    if p_start_index < len(test_paragraphs_tokenized[paragraph_id]):\n","                        # start_range = test_paragraphs_tokenized[paragraph_id].token_to_chars(p_start_index)\n","                        if p_start_index != 0:\n","                            start = test_paragraphs_tokenized[paragraph_id].token_to_chars(p_start_index)[0]\n","                        else:\n","                            start = 0\n","                        if end_index > 193-question_len+1:\n","                            continue\n","                        else:\n","                            end = test_paragraphs_tokenized[paragraph_id].token_to_chars(p_end_index)[1]\n","\n","                        answer = test_paragraphs[paragraph_id][start : end]\n","\n","    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n","    return answer.replace(' ','')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rzHQit6eMnKG"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"3Q-B6ka7xoCM","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5cc0f2a75a72493eb4957b8f61c2993c","ef0db578f8414ec381ff35cc2ee8b0c1","d988337970e84a42b0286ebaae517dc5","85d15fe0d7b74bdca99960a4f93d9ba9","ceee4c67fc8d49909faa8d6364a18c92","b13c8958e7a14679aaa036967ec5e0e9","e2385d7872954bc6913c67c3f5894aab","6807ba06bd7441fe862996ae7c7530e4"]},"executionInfo":{"status":"error","timestamp":1621659337605,"user_tz":-480,"elapsed":117467,"user":{"displayName":"李亭臻","photoUrl":"","userId":"05946528128840806794"}},"outputId":"57520051-2b99-4a6b-8a41-318be5d4fe49"},"source":["import transformers\n","num_epoch = 5 \n","validation = False\n","logging_step = 100\n","learning_rate = 3e-5\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","scheduler = transformers.get_linear_schedule_with_warmup(optimizer, 500, 5000)\n","if fp16_training:\n","    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n","\n","model.train()\n","\n","print(\"Start Training ...\")\n","\n","for epoch in range(num_epoch):\n","    step = 1\n","    train_loss = train_acc = 0\n","    \n","    for data in tqdm(train_loader):\t\n","        # Load all data into GPU\n","        # data = [i.to(device) for i in data]\n","        \n","        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n","        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)\n","        # print(\"data[0]_shape:\", data[0].shape)\n","        # print(\"data[0]_squeeze_shape:\", data[0].squeeze(dim=0).shape)\n","        # output = model(input_ids=data[0].squeeze(dim=0).to(device),\n","        #                 token_type_ids=data[1].squeeze(dim=0).to(device),\n","        #                 attention_mask=data[2].squeeze(dim=0).to(device),\n","        #                 start_positions=data[3].squeeze(dim=0).to(device),\n","        #                 end_positions=data[4].squeeze(dim=0).to(device))\n","        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n","\n","        # Choose the most probable start position / end position\n","        start_index = torch.argmax(output.start_logits, dim=1)\n","        end_index = torch.argmax(output.end_logits, dim=1)\n","        \n","        # Prediction is correct only if both start_index and end_index are correct\n","        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n","        train_loss += output.loss\n","        \n","        if fp16_training:\n","            accelerator.backward(output.loss)\n","        else:\n","            output.loss.backward()\n","\n","        \n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        step += 1\n","\n","        ##### TODO: Apply linear learning rate decay #####\n","        \n","        # Print training loss and accuracy over past logging step\n","        if step % 100 == 0:\n","            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n","            train_loss = train_acc = 0\n","    \n","    # if validation:\n","    #     print(\"Evaluating Dev Set ...\")\n","    #     model.eval()\n","    #     with torch.no_grad():\n","    #         dev_acc = 0\n","    #         for i, data in enumerate(tqdm(dev_loader)):\n","    #             output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","    #                    attention_mask=data[2].squeeze(dim=0).to(device))\n","    #             # prediction is correct only if answer text exactly matches\n","    #             dev_acc += evaluate(data, output) == dev_questions[i][\"answer_text\"]\n","    #         print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n","    #     model.train()\n","    # step = 1\n","    # train_loss = train_acc = 0\n","    # for data in tqdm(dev_loader):\t\n","    #     # Load all data into GPU\n","    #     data = [i.to(device) for i in data]\n","    #     # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n","    #     # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)\n","    #     output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n","\n","    #     # Choose the most probable start position / end position\n","    #     start_index = torch.argmax(output.start_logits, dim=1)\n","    #     end_index = torch.argmax(output.end_logits, dim=1)\n","        \n","    #     # Prediction is correct only if both start_index and end_index are correct\n","    #     train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n","    #     train_loss += output.loss\n","        \n","    #     if fp16_training:\n","    #         accelerator.backward(output.loss)\n","    #     else:\n","    #         output.loss.backward()\n","\n","    #     optimizer.step()\n","    #     scheduler.step()\n","    \n","    \n","    #     optimizer.zero_grad()\n","    #     step += 1\n","\n","    #     ##### TODO: Apply linear learning rate decay #####\n","        \n","    #     # Print training loss and accuracy over past logging step\n","        # if step % 20 == 0:\n","        #     print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n","        #     train_loss = train_acc = 0\n","# Save a model and its configuration file to the directory 「saved_model」 \n","# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n","# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n","print(\"Saving Model ...\")\n","model_name = 'chinese-macbert-large-with-dev'\n","model.save_pretrained(f'{workspace_dir}/' + model_name)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Start Training ...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cc0f2a75a72493eb4957b8f61c2993c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=26936.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","Epoch 1 | Step 20 | loss = 1.050, acc = 0.000\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","Epoch 1 | Step 40 | loss = 1.072, acc = 0.000\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n","data[0]_shape: torch.Size([1, 3, 193])\n","data[0]_squeeze_shape: torch.Size([3, 193])\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-e526ac536ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Load all data into GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# data = [i.to(device) for i in data]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0msynchronize_rng_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAcceleratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"kMmdLOKBMsdE"},"source":["## Testing"]},{"cell_type":"code","metadata":{"id":"U5scNKC9xz0C"},"source":["print(\"Evaluating Test Set ...\")\n","\n","result = []\n","\n","model.eval()\n","count = 0\n","with torch.no_grad():\n","    for data in tqdm(test_loader):\n","        if count > 99999:\n","            output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","                        attention_mask=data[2].squeeze(dim=0).to(device))\n","        \n","            result.append(evaluate(data, output, count, True))\n","            break\n","        else:\n","            output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n","                        attention_mask=data[2].squeeze(dim=0).to(device))\n","            result.append(evaluate(data, output, count))\n","        count = count + 1\n","\n","result_file = \"macbert_with_dev.csv\"\n","with open(result_file, 'w') as f:\t\n","    f.write(\"ID,Answer\\n\")\n","    for i, test_question in enumerate(test_questions):\n","        # Replace commas in answers with empty strings (since csv is separated by comma)\n","        # Answers in kaggle are processed in the same way\n","        f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n","\n","\n","print(f\"Completed! Result is in {result_file}\")"],"execution_count":null,"outputs":[]}]}