{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"hw6_GAN.ipynb","provenance":[{"file_id":"https://github.com/ga642381/ML2021-Spring/blob/main/HW06/HW06.ipynb","timestamp":1619804121736}],"collapsed_sections":["XkvZ4JgCHCZD","MV6ApRWxq_7V","eMiJNo-dH27c"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oZ-C2Dgetg37"},"source":["# **Homework 6 - Generative Adversarial Network**\n","\n","This is the example code of homework 6 of the machine learning course by Prof. Hung-yi Lee.\n","\n","\n","In this homework, you are required to build a generative adversarial  network for anime face generation.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvvqoVs8KvB6","executionInfo":{"status":"ok","timestamp":1620399237722,"user_tz":-480,"elapsed":1202,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}},"outputId":"8b55f417-6183-48a9-9778-2a4ed7aef58d"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Fri May  7 14:53:56 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5lzZZu_znMM1","executionInfo":{"status":"ok","timestamp":1620399356664,"user_tz":-480,"elapsed":29937,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}},"outputId":"532a8b4e-2480-4474-c760-c0e868d7ea28"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JTBkY5QFf3QM"},"source":["## Set up the environment\n"]},{"cell_type":"markdown","metadata":{"id":"Y7y4wyYdEABR"},"source":["### Packages Installation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6IQB485dD_eL","executionInfo":{"status":"ok","timestamp":1620399311538,"user_tz":-480,"elapsed":6919,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}},"outputId":"973a007f-04ae-4381-e32d-592b1d2cfc99"},"source":["# You may replace the workspace directory if you want.\n","workspace_dir = '/content/drive/MyDrive/ML2021-hw6'\n","\n","# Training progress bar\n","!pip install -q qqdm\n","!pip install Path\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["  Building wheel for qqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Path\n","  Downloading https://files.pythonhosted.org/packages/d3/2a/b0f97e1b736725f6ec48a8bd564ee1d1f3f945bb5d39cb44ef8bbe66bd14/path-15.1.2-py3-none-any.whl\n","Installing collected packages: Path\n","Successfully installed Path-15.1.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2NFjuZTPDxLn"},"source":["### Download the dataset\n","**Please use the link according to the last digit of your student ID first!**\n","\n","If all download links fail, please follow [here](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e).\n","\n","* To open the file using your browser, use the link below (replace the id first!):\n","https://drive.google.com/file/d/REPLACE_WITH_ID\n","* e.g. https://drive.google.com/file/d/1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p"]},{"cell_type":"code","metadata":{"id":"uZomvVA2f607"},"source":["# !gdown --id 1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# Other download links\n","#   Please uncomment the line according to the last digit of your student ID first\n","\n","# 0\n","# !gdown --id 131zPaVoi-U--XThvzgRfaxrumc3YSBd3 --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 1\n","# !gdown --id 1kCuIj1Pf3T2O94H9bUBxjPBKb---WOmH --output \"./crypko_data.zip\"\n","\n","# 2\n","# !gdown --id 1boEoiiqBJwoHVvjmI0xgoutE9G0Rv8CD --output \"./crypko_data.zip\"\n","\n","# 3\n","# !gdown --id 1Ic0mktAQQvnNAnswrPHsg-u2OWGBXTWF --output \"./crypko_data.zip\"\n","\n","# 4\n","# !gdown --id 1PFcc25r9tLE7OyQ-CDadtysNdWizk6Yg --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 5\n","# !gdown --id 1wgkrYkTrhwDSMdWa5NwpXeE4-7JaUuX2 --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 6\n","# !gdown --id 19gwNYWi9gN9xVL86jC3v8qqNtrXyq5Bf --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 7 \n","# !gdown --id 1-KPZB6frRSRLRAtQfafKCVA7em0_NrJG --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 8\n","# !gdown --id 1rNBfmn0YBzXuG5ub7CXbsGwduZqEs8hx --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 9\n","# !gdown --id 113NEISX-2j6rBd1yyBx0c3_9nPIzSNz- --output \"{workspace_dir}/crypko_data.zip\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pNtT1WCOyRNt"},"source":["###Unzip the downloaded file.\n","The unzipped tree structure is like \n","```\n","faces/\n","├── 1.jpg\n","├── 2.jpg\n","├── 3.jpg\n","...\n","```"]},{"cell_type":"code","metadata":{"id":"s2qR-0hjqWE6","executionInfo":{"status":"ok","timestamp":1620399373537,"user_tz":-480,"elapsed":11895,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}}},"source":["!unzip -q \"/content/drive/MyDrive/ML2021-hw6/crypko_data.zip\" -d \"./\""],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjfM46dtmxXj"},"source":["## Random seed\n","Set the random seed to a certain value for reproducibility."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"abH5_iQcMPvA","executionInfo":{"status":"error","timestamp":1620279764116,"user_tz":-480,"elapsed":967,"user":{"displayName":"Tammy Lee","photoUrl":"","userId":"02830711679030502652"}},"outputId":"cc86d140-b8a5-4b1d-ef53-5586a8edace9"},"source":["import os\n","test = os.path.join(tempfile.gettempdir(), 'torch_extensions', 'fused')\n","test"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ffcfa15f6702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgettempdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'torch_extensions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fused'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tempfile' is not defined"]}]},{"cell_type":"code","metadata":{"id":"OWuecW1imz42"},"source":["import random\n","\n","import torch\n","import numpy as np\n","\n","\n","def same_seeds(seed):\n","    # Python built-in random module\n","    random.seed(seed)\n","    # Numpy\n","    np.random.seed(seed)\n","    # Torch\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","same_seeds(2021)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCTPz2iRQmwe"},"source":["## Import Packages\n","First, we need to import packages that will be used later.\n","\n","Like hw3, we highly rely on **torchvision**, a library of PyTorch."]},{"cell_type":"code","metadata":{"id":"TC8RRsX0QhL-","executionInfo":{"status":"ok","timestamp":1620434182796,"user_tz":-480,"elapsed":4749,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}}},"source":["import os\n","import glob\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch import optim\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","# from qqdm.notebook import qqdm\n","from argparse import Namespace\n","from pathlib import Path\n","from copy import deepcopy\n","from tqdm import tqdm\n","from torch.autograd import grad as torch_grad"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kYjZ_G83_YX4"},"source":["## Dataset\n","1. Resize the images to (64, 64)\n","1. Linearly map the values from [0, 1] to  [-1, 1].\n","\n","Please refer to [PyTorch official website](https://pytorch.org/vision/stable/transforms.html) for details about different transforms.\n"]},{"cell_type":"code","metadata":{"id":"UZ6d0_cr8R26"},"source":["class CrypkoDataset(Dataset):\n","    def __init__(self, fnames, transform):\n","        self.transform = transform\n","        self.fnames = fnames\n","        self.num_samples = len(self.fnames)\n","\n","    def __getitem__(self,idx):\n","        fname = self.fnames[idx]\n","        # 1. Load the image\n","        img = torchvision.io.read_image(fname)\n","        # 2. Resize and normalize the images using torchvision.\n","        img = self.transform(img)\n","        return img\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","\n","def get_dataset(root):\n","    fnames = glob.glob(os.path.join(root, '*'))\n","    # 1. Resize the image to (64, 64)\n","    # 2. Linearly map [0, 1] to [-1, 1]\n","    compose = [\n","        transforms.ToPILImage(),\n","        transforms.Resize((64, 64)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n","    ]\n","    transform = transforms.Compose(compose)\n","    dataset = CrypkoDataset(fnames, transform)\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tGwdVhOKSjLY"},"source":["### Show some images\n","Note that the values are in the range of [-1, 1], we should shift them to the valid range, [0, 1], to display correctly."]},{"cell_type":"code","metadata":{"id":"34mVNtHn7cwF"},"source":["dataset = get_dataset(os.path.join('/content/', 'faces'))\n","\n","images = [dataset[i] for i in range(16)]\n","grid_img = torchvision.utils.make_grid(images, nrow=4)\n","plt.figure(figsize=(10,10))\n","plt.imshow(grid_img.permute(1, 2, 0))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DxP7LoZjLI2n"},"source":["len(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nhxUjRUuHdti"},"source":["images = [(dataset[i]+1)/2 for i in range(16)]\n","grid_img = torchvision.utils.make_grid(images, nrow=4)\n","plt.figure(figsize=(10,10))\n","plt.imshow(grid_img.permute(1, 2, 0))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XkvZ4JgCHCZD"},"source":["## Model\n","Here, we use DCGAN as the model structure. Feel free to modify your own model structure.\n","\n","Note that the `N` of the input/output shape stands for the batch size."]},{"cell_type":"code","metadata":{"id":"F0I1jRd6HFmm"},"source":["# def weights_init(m):\n","#     classname = m.__class__.__name__\n","#     if classname.find('Conv') != -1:\n","#         m.weight.data.normal_(0.0, 0.02)\n","#     elif classname.find('BatchNorm') != -1:\n","#         m.weight.data.normal_(1.0, 0.02)\n","#         m.bias.data.fill_(0)\n","\n","# class Generator(nn.Module):\n","#     \"\"\"\n","#     Input shape: (N, in_dim)\n","#     Output shape: (N, 3, 64, 64)\n","#     \"\"\"\n","#     def __init__(self, in_dim, dim=64):\n","#         super(Generator, self).__init__()\n","#         def dconv_bn_relu(in_dim, out_dim):\n","#             return nn.Sequential(\n","#                 nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n","#                                    padding=2, output_padding=1, bias=False),\n","#                 nn.BatchNorm2d(out_dim),\n","#                 nn.ReLU()\n","#             )\n","#         self.l1 = nn.Sequential(\n","#             nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n","#             nn.BatchNorm1d(dim * 8 * 4 * 4),\n","#             nn.ReLU()\n","#         )\n","#         self.l2_5 = nn.Sequential(\n","#             dconv_bn_relu(dim * 8, dim * 4),\n","#             dconv_bn_relu(dim * 4, dim * 2),\n","#             dconv_bn_relu(dim * 2, dim),\n","#             nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),\n","#             nn.Tanh()\n","#         )\n","#         self.apply(weights_init)\n","\n","#     def forward(self, x):\n","#         y = self.l1(x)\n","#         y = y.view(y.size(0), -1, 4, 4)\n","#         y = self.l2_5(y)\n","#         return y\n","\n","# class Discriminator(nn.Module):\n","#     \"\"\"\n","#     Input shape: (N, 3, 64, 64)\n","#     Output shape: (N, )\n","#     \"\"\"\n","#     def __init__(self, in_dim, dim=64):\n","#         super(Discriminator, self).__init__()\n","\n","#         def conv_bn_lrelu(in_dim, out_dim):\n","#             return nn.Sequential(\n","#                 nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n","#                 nn.BatchNorm2d(out_dim),\n","#                 nn.LeakyReLU(0.2),\n","#             )\n","            \n","#         \"\"\" Medium: Remove the last sigmoid layer for WGAN. \"\"\"\n","#         self.ls = nn.Sequential(\n","#             nn.Conv2d(in_dim, dim, 5, 2, 2), \n","#             nn.LeakyReLU(0.2),\n","#             conv_bn_lrelu(dim, dim * 2),\n","#             conv_bn_lrelu(dim * 2, dim * 4),\n","#             conv_bn_lrelu(dim * 4, dim * 8),\n","#             nn.Conv2d(dim * 8, 1, 4), \n","#         )\n","#         self.apply(weights_init)\n","        \n","#     def forward(self, x):\n","#         y = self.ls(x)\n","#         y = y.view(-1)\n","#         return y\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MV6ApRWxq_7V"},"source":["## SA-GAN\n"]},{"cell_type":"code","metadata":{"id":"qZ7bHcuhrGoi","executionInfo":{"status":"ok","timestamp":1620434184604,"user_tz":-480,"elapsed":998,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}}},"source":["import torch\n","from torch.optim.optimizer import Optimizer, required\n","\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torch import nn\n","from torch import Tensor\n","from torch.nn import Parameter\n","\n","def l2normalize(v, eps=1e-12):\n","    return v / (v.norm() + eps)\n","\n","\n","class SpectralNorm(nn.Module):\n","    def __init__(self, module, name='weight', power_iterations=1):\n","        super(SpectralNorm, self).__init__()\n","        self.module = module\n","        self.name = name\n","        self.power_iterations = power_iterations\n","        if not self._made_params():\n","            self._make_params()\n","\n","    def _update_u_v(self):\n","        u = getattr(self.module, self.name + \"_u\")\n","        v = getattr(self.module, self.name + \"_v\")\n","        w = getattr(self.module, self.name + \"_bar\")\n","\n","        height = w.data.shape[0]\n","        for _ in range(self.power_iterations):\n","            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n","            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n","\n","        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n","        sigma = u.dot(w.view(height, -1).mv(v))\n","        setattr(self.module, self.name, w / sigma.expand_as(w))\n","\n","    def _made_params(self):\n","        try:\n","            u = getattr(self.module, self.name + \"_u\")\n","            v = getattr(self.module, self.name + \"_v\")\n","            w = getattr(self.module, self.name + \"_bar\")\n","            return True\n","        except AttributeError:\n","            return False\n","\n","\n","    def _make_params(self):\n","        w = getattr(self.module, self.name)\n","\n","        height = w.data.shape[0]\n","        width = w.view(height, -1).data.shape[1]\n","\n","        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n","        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n","        u.data = l2normalize(u.data)\n","        v.data = l2normalize(v.data)\n","        w_bar = Parameter(w.data)\n","\n","        del self.module._parameters[self.name]\n","\n","        self.module.register_parameter(self.name + \"_u\", u)\n","        self.module.register_parameter(self.name + \"_v\", v)\n","        self.module.register_parameter(self.name + \"_bar\", w_bar)\n","\n","\n","    def forward(self, *args):\n","        self._update_u_v()\n","        return self.module.forward(*args)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"FryeDrPArCkC","executionInfo":{"status":"ok","timestamp":1620434185124,"user_tz":-480,"elapsed":1514,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","\n","class Self_Attn(nn.Module):\n","    \"\"\" Self attention Layer\"\"\"\n","    def __init__(self,in_dim,activation):\n","        super(Self_Attn,self).__init__()\n","        self.chanel_in = in_dim\n","        self.activation = activation\n","        \n","        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n","        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n","        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","        self.softmax  = nn.Softmax(dim=-1) #\n","    def forward(self,x):\n","        \"\"\"\n","            inputs :\n","                x : input feature maps( B X C X W X H)\n","            returns :\n","                out : self attention value + input feature \n","                attention: B X N X N (N is Width*Height)\n","        \"\"\"\n","        m_batchsize,C,width ,height = x.size()\n","        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n","        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n","        energy =  torch.bmm(proj_query,proj_key) # transpose check\n","        attention = self.softmax(energy) # BX (N) X (N) \n","        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n","\n","        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n","        out = out.view(m_batchsize,C,width,height)\n","        \n","        out = self.gamma*out + x\n","        return out,attention\n","\n","class Generator(nn.Module):\n","    \"\"\"Generator.\"\"\"\n","\n","    def __init__(self, batch_size, image_size=64, z_dim=100, conv_dim=64):\n","        super(Generator, self).__init__()\n","        self.imsize = image_size\n","        layer1 = []\n","        layer2 = []\n","        layer3 = []\n","        last = []\n","\n","        repeat_num = int(np.log2(self.imsize)) - 3\n","        mult = 2 ** repeat_num # 8\n","        layer1.append(SpectralNorm(nn.ConvTranspose2d(z_dim, conv_dim * mult, 4)))\n","        layer1.append(nn.BatchNorm2d(conv_dim * mult))\n","        layer1.append(nn.ReLU())\n","\n","        curr_dim = conv_dim * mult\n","\n","        layer2.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n","        layer2.append(nn.BatchNorm2d(int(curr_dim / 2)))\n","        layer2.append(nn.ReLU())\n","\n","        curr_dim = int(curr_dim / 2)\n","\n","        layer3.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n","        layer3.append(nn.BatchNorm2d(int(curr_dim / 2)))\n","        layer3.append(nn.ReLU())\n","\n","        if self.imsize == 64:\n","            layer4 = []\n","            curr_dim = int(curr_dim / 2)\n","            layer4.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n","            layer4.append(nn.BatchNorm2d(int(curr_dim / 2)))\n","            layer4.append(nn.ReLU())\n","            self.l4 = nn.Sequential(*layer4)\n","            curr_dim = int(curr_dim / 2)\n","\n","        self.l1 = nn.Sequential(*layer1)\n","        self.l2 = nn.Sequential(*layer2)\n","        self.l3 = nn.Sequential(*layer3)\n","\n","        last.append(nn.ConvTranspose2d(curr_dim, 3, 4, 2, 1))\n","        last.append(nn.Tanh())\n","        self.last = nn.Sequential(*last)\n","\n","        self.attn1 = Self_Attn( 128, 'relu')\n","        self.attn2 = Self_Attn( 64,  'relu')\n","\n","    def forward(self, z):\n","        z = z.view(z.size(0), z.size(1), 1, 1)\n","        out=self.l1(z)\n","        out=self.l2(out)\n","        out=self.l3(out)\n","        out,p1 = self.attn1(out)\n","        out=self.l4(out)\n","        out,p2 = self.attn2(out)\n","        out=self.last(out)\n","\n","        return out, p1, p2\n","\n","\n","class Discriminator(nn.Module):\n","    \"\"\"Discriminator, Auxiliary Classifier.\"\"\"\n","\n","    def __init__(self, batch_size=64, image_size=64, conv_dim=64):\n","        super(Discriminator, self).__init__()\n","        self.imsize = image_size\n","        layer1 = []\n","        layer2 = []\n","        layer3 = []\n","        last = []\n","\n","        layer1.append(SpectralNorm(nn.Conv2d(3, conv_dim, 4, 2, 1)))\n","        layer1.append(nn.LeakyReLU(0.1))\n","\n","        curr_dim = conv_dim\n","\n","        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n","        layer2.append(nn.LeakyReLU(0.1))\n","        curr_dim = curr_dim * 2\n","\n","        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n","        layer3.append(nn.LeakyReLU(0.1))\n","        curr_dim = curr_dim * 2\n","\n","        if self.imsize == 64:\n","            layer4 = []\n","            layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n","            layer4.append(nn.LeakyReLU(0.1))\n","            self.l4 = nn.Sequential(*layer4)\n","            curr_dim = curr_dim*2\n","        self.l1 = nn.Sequential(*layer1)\n","        self.l2 = nn.Sequential(*layer2)\n","        self.l3 = nn.Sequential(*layer3)\n","\n","        last.append(nn.Conv2d(curr_dim, 1, 4))\n","        self.last = nn.Sequential(*last)\n","\n","        self.attn1 = Self_Attn(256, 'relu')\n","        self.attn2 = Self_Attn(512, 'relu')\n","\n","    def forward(self, x):\n","        out = self.l1(x)\n","        out = self.l2(out)\n","        out = self.l3(out)\n","        out,p1 = self.attn1(out)\n","        out=self.l4(out)\n","        out,p2 = self.attn2(out)\n","        out=self.last(out)\n","\n","        return out.squeeze(), p1, p2"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMiJNo-dH27c"},"source":["## Adam + Lr Scheduling"]},{"cell_type":"code","metadata":{"id":"vwA6bfD8H9NC"},"source":["class NoamOpt:\n","    \"Optim wrapper that implements rate.\"\n","    def __init__(self, model_size, factor, warmup, optimizer):\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self.warmup = warmup\n","        self.factor = factor\n","        self.model_size = model_size\n","        self._rate = 0\n","    \n","    property\n","    def param_groups(self):\n","        return self.optimizer.param_groups\n","        \n","    def multiply_grads(self, c):\n","        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is not None:\n","                    p.grad.data.mul_(c)\n","        \n","    def step(self):\n","        \"Update parameters and rate\"\n","        self._step += 1\n","        rate = self.rate()\n","        for p in self.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","        self.optimizer.step()\n","        \n","    def rate(self, step = None):\n","        \"Implement `lrate` above\"\n","        if step is None:\n","            step = self._step\n","        return 0 if not step else self.factor * \\\n","            (self.model_size ** (-0.5) *\n","            min(step ** (-0.5), step * self.warmup ** (-1.5)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cxo4teqaO5RJ"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"v5sCGIUtSViC"},"source":["### Initialization\n","- hyperparameters\n","- model\n","- optimizer\n","- dataloader"]},{"cell_type":"code","metadata":{"id":"m6QbX0pQstC1","executionInfo":{"status":"ok","timestamp":1620435005173,"user_tz":-480,"elapsed":721,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}}},"source":["config = Namespace(\n","    model = 'sagan',\n","    imsize = 64,\n","    g_num = 5,\n","    z_dim = 128,\n","    g_conv_dim = 64,\n","    d_conv_dim = 64,\n","    lambda_gp = 10,\n","    version = 'sagan_1',\n","\n",")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"2EqomOouHezf"},"source":["# Training hyperparameters\n","batch_size = 64\n","z_dim = 128\n","z_sample = Variable(torch.randn(n_output, config.z_dim)).cuda()\n","lr = 0.0002\n","\n","\"\"\" Medium: WGAN, 50 epoch, n_critic=5, clip_value=0.01 \"\"\"\n","start_epoch = 100\n","n_epoch = 500 # 50\n","n_critic = 5 # 5\n","# clip_value = 0.01\n","\n"," \n","log_dir = os.path.join(workspace_dir, 'logs')\n","ckpt_dir = os.path.join(workspace_dir, 'checkpoints')\n","os.makedirs(log_dir, exist_ok=True)\n","os.makedirs(ckpt_dir, exist_ok=True)\n","\n","# Model\n","\n","\n","G = Generator(batch_size,config.imsize, config.z_dim, config.g_conv_dim).cuda()\n","D = Discriminator(batch_size,config.imsize, config.d_conv_dim).cuda()\n","G.train()\n","D.train()\n","# Loss\n","# criterion = nn.BCELoss()\n","\n","\"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n","# Optimizer\n","# opt_D = NoamOpt(\n","#     model_size=config.z_dim, \n","#     factor=config.lr_factor, \n","#     warmup=config.lr_warmup, \n","#     optimizer=torch.optim.Adam(D.parameters(), lr=0.005, betas=(0.5, 0.999)))\n","# opt_G = NoamOpt(\n","#     model_size=config.z_dim, \n","#     factor=config.lr_factor, \n","#     warmup=config.lr_warmup, \n","#     optimizer=torch.optim.Adam(G.parameters(), lr=0.001, betas=(0.5, 0.999)))\n","opt_D = torch.optim.Adam(D.parameters(), lr=0.005, betas=(0.5, 0.999))\n","opt_G = torch.optim.Adam(G.parameters(), lr=0.001, betas=(0.5, 0.999))\n","# opt_D = torch.optim.RMSprop(D.parameters(), lr=lr)\n","# opt_G = torch.optim.RMSprop(G.parameters(), lr=lr)\n","\n","# DataLoader\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k8YPNqYIq0Kf"},"source":["### Load & Save\n","\n"]},{"cell_type":"code","metadata":{"id":"qX410h0Bq6s1"},"source":["def save(model_G, model_D, optimizer_G, optimizer_D, epoch,save=True):\n","  if save:\n","        # save epoch checkpoint\n","        savedir = Path(ckpt_dir).absolute()\n","        savedir.mkdir(parents=True, exist_ok=True)\n","        \n","        check_G = {\n","            \"model\": model_G.state_dict(),\n","            \"optim\": optimizer_G.state_dict(), \n","        }\n","        torch.save(check_G, savedir/f\"checkpointG_{config.model}_{epoch}.pth\")\n","\n","        check_D = {\n","            \"model\": model_D.state_dict(),\n","            \"optim\": optimizer_D.state_dict()\n","        }\n","        torch.save(check_D, savedir/f\"checkpointD_{config.model}_{epoch}.pth\")\n","        print(\"saved epoch \" + str(epoch) +\" checkpoint\")\n","\n","def load_checkpoint(model_G, model_D, optimizer_G, optimizer_D):\n","    checkpath_G = Path(ckpt_dir)/f\"checkpointG_{config.model}_{start_epoch-1}.pth\"\n","    if checkpath_G.exists():\n","        check_G = torch.load(checkpath_G)\n","        model_G.load_state_dict(check_G[\"model\"])\n","        optimizer_G.load_state_dict(check_G[\"optim\"])\n","        print(\"loaded checkpointG\")\n","    else:\n","        print(\"no checkpointG\")\n","\n","    checkpath_D = Path(ckpt_dir)/f\"checkpointD_{config.model}_{start_epoch-1}.pth\"\n","    if checkpath_D.exists():\n","        check_D = torch.load(checkpath_D)\n","        model_D.load_state_dict(check_D[\"model\"])\n","        optimizer_D.load_state_dict(check_D[\"optim\"])\n","        print(\"loaded checkpointD\")\n","    else:\n","        print(\"no checkpointD\")\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QV4nv5GtDOkg"},"source":["def gradient_penalty(real_data, generated_data):\n","    batch_size = real_data.size()[0]\n","\n","    # Calculate interpolation\n","    alpha = torch.rand(batch_size, 1, 1, 1)\n","    alpha = alpha.expand_as(real_data)\n","\n","    alpha = alpha.cuda()\n","    interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n","    interpolated = Variable(interpolated, requires_grad=True)\n","\n","    interpolated = interpolated.cuda()\n","\n","    # Calculate probability of interpolated examples\n","    prob_interpolated,_,_  = D(interpolated)\n","\n","    # Calculate gradients of probabilities with respect to examples\n","    gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n","                            grad_outputs=torch.ones(prob_interpolated.size()).cuda(),\n","                            create_graph=True, retain_graph=True)[0]\n","\n","    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n","    # so flatten to easily take norm per example in batch\n","    gradients = gradients.view(batch_size, -1)\n","\n","    # Derivatives of the gradient close to 0 can cause problems because of\n","    # the square root, so manually calculate norm and add epsilon\n","    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n","\n","    # Return gradient penalty\n","    return 10 * torch.mean(((gradients_norm - 1) ** 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vpJA1wzi0tii"},"source":["### Training loop\n","We store some pictures regularly to monitor the current \bperformance of the Generator, and regularly record checkpoints."]},{"cell_type":"code","metadata":{"id":"dgkqPih1o5Az"},"source":["steps = 0\n","load_checkpoint(G, D, opt_G, opt_D)\n","for epoch in range(start_epoch, n_epoch+1):\n","    progress_bar = qqdm(dataloader)\n","    for i, data in enumerate(progress_bar):\n","        imgs = data\n","        imgs = imgs.cuda()\n","\n","        bs = imgs.size(0)\n","\n","        # ============================================\n","        #  Train D\n","        # ============================================\n","        \n","        z = Variable(torch.randn(bs, z_dim)).cuda()\n","        r_imgs = Variable(imgs).cuda()\n","        f_imgs,_,_  = G(z)\n","\n","        # penalty = gradient_penalty(r_imgs, f_imgs)\n","\n","        \"\"\" Medium: Use WGAN Loss. \"\"\"\n","        # Label\n","        # r_label = torch.ones((bs)).cuda()\n","        # f_label = torch.zeros((bs)).cuda()\n","\n","        # Model forwarding\n","        # r_logit = D(r_imgs.detach())\n","        # f_logit = D(f_imgs.detach())\n","        \n","        # Compute the loss for the discriminator.\n","        # r_loss = criterion(r_logit, r_label)\n","        # f_loss = criterion(f_logit, f_label)\n","        # loss_D = (r_loss + f_loss) / 2\n","\n","        # WGAN Loss\n","        # loss_D = -torch.mean(D(r_imgs)) + torch.mean(D(f_imgs)) + penalty\n","        fake,_,_ = D(f_imgs)\n","        real,_,_ = D(r_imgs)\n","        loss_D = torch.mean(nn.ReLU(inplace=True)(1.0 + fake)) + torch.mean(nn.ReLU(inplace=True)(1.0 - real))\n","\n","        # Model backwarding\n","        D.zero_grad()\n","        loss_D.backward()\n","\n","        # Update the discriminator.\n","        opt_D.step()\n","\n","        \"\"\" Medium: Clip weights of discriminator. \"\"\"\n","        # for p in D.parameters():\n","        #    p.data.clamp_(-clip_value, clip_value)\n","\n","        # ============================================\n","        #  Train G\n","        # ============================================\n","        # if steps % n_critic == 0:\n","            # Generate some fake images.\n","            \n","\n","        z = Variable(torch.randn(bs, z_dim)).cuda()\n","        f_imgs,_,_  = G(z)\n","\n","        # Model forwarding\n","        # f_logit = D(f_imgs)\n","        \n","        \"\"\" Medium: Use WGAN Loss\"\"\"\n","        # Compute the loss for the generator.\n","        # loss_G = criterion(f_logit, r_label)\n","        # WGAN Loss\n","        # loss_G = -torch.mean(D(f_imgs))\n","        fake,_,_ = D(f_imgs)\n","        loss_G = -torch.mean(fake)\n","        \n","        # Model backwarding\n","        G.zero_grad()\n","        loss_G.backward()\n","\n","        # Update the generator.\n","        opt_G.step()\n","\n","        steps += 1\n","        \n","        # Set the info of the progress bar\n","        #   Note that the value of the GAN loss is not directly related to\n","        #   the quality of the generated images.\n","        progress_bar.set_infos({\n","            'Loss_D': round(loss_D.item(), 4),\n","            'Loss_G': round(loss_G.item(), 4),\n","            'Epoch': epoch,\n","            'Step': steps,\n","        })\n","\n","    G.eval()\n","    fake,_,_ = G(z_sample)\n","    f_imgs_sample = (fake.data + 1) / 2.0\n","    filename = os.path.join(log_dir, f'Epoch_{epoch:03d}.jpg')\n","    torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n","    print(f' | Save some samples to {filename}.')\n","    \n","    # Show generated images in the jupyter notebook.\n","    grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n","    plt.figure(figsize=(10,10))\n","    plt.imshow(grid_img.permute(1, 2, 0))\n","    plt.show()\n","    G.train()\n","\n","    save(G, D, opt_G, opt_D, epoch)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2uJFmTtKBeH"},"source":["## Inference\n","Use the trained model to generate anime faces!"]},{"cell_type":"markdown","metadata":{"id":"tXPXcVD_HJB2"},"source":["### Load model "]},{"cell_type":"code","metadata":{"id":"4JnQdNx2SUS2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620435010674,"user_tz":-480,"elapsed":695,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}},"outputId":"e1b82bc9-7739-418a-d05d-81792c7c8076"},"source":["import torch\n","checkpath_G = \"/content/checkpointG_sagan_220.pth\"\n","check_G = torch.load(checkpath_G)\n","G = Generator(64,config.imsize, config.z_dim, config.g_conv_dim).cuda()\n","G.load_state_dict(check_G[\"model\"])\n","G.eval()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Generator(\n","  (l4): Sequential(\n","    (0): SpectralNorm(\n","      (module): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    )\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (l1): Sequential(\n","    (0): SpectralNorm(\n","      (module): ConvTranspose2d(128, 512, kernel_size=(4, 4), stride=(1, 1))\n","    )\n","    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (l2): Sequential(\n","    (0): SpectralNorm(\n","      (module): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    )\n","    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (l3): Sequential(\n","    (0): SpectralNorm(\n","      (module): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    )\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (last): Sequential(\n","    (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): Tanh()\n","  )\n","  (attn1): Self_Attn(\n","    (query_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n","    (key_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n","    (value_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (softmax): Softmax(dim=-1)\n","  )\n","  (attn2): Self_Attn(\n","    (query_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n","    (key_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n","    (value_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (softmax): Softmax(dim=-1)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"-I8PDocbHQiN"},"source":["### Generate and show some images.\n"]},{"cell_type":"code","metadata":{"id":"x-SYKrRea_-Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620435018580,"user_tz":-480,"elapsed":1444,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}},"outputId":"0bbac894-051e-430f-fa0f-394668600565"},"source":["# Generate 1000 images and make a grid to save them.\n","n_output = 1000\n","z_sample = Variable(torch.randn(n_output, config.z_dim)).cuda()\n","fake,_,_ = G(z_sample)\n","imgs_sample = (fake.data + 1) / 2.0\n","print(type(imgs_sample))\n","torchvision.utils.save_image(imgs_sample, '/content/result.jpg', nrow=10)\n","\n","# Show 32 of the images.\n","# grid_img = torchvision.utils.make_grid(imgs_sample[:32].cpu(), nrow=10)\n","# plt.figure(figsize=(10,10))\n","# plt.imshow(grid_img.permute(1, 2, 0))\n","# plt.show()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["<class 'torch.Tensor'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whsnu-GYdFfx","executionInfo":{"status":"ok","timestamp":1620435044902,"user_tz":-480,"elapsed":852,"user":{"displayName":"李亭臻","photoUrl":"","userId":"04072126738796809743"}},"outputId":"e3a1e416-cff0-4d10-f06d-3049174fc09e"},"source":["imgs_sample.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1000, 3, 64, 64])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"6B04ATOTHc4F"},"source":["### Compress the generated images using **tar**.\n"]},{"cell_type":"code","metadata":{"id":"mbcmoTQpz_yf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620377434572,"user_tz":-480,"elapsed":1391,"user":{"displayName":"Tammy Lee","photoUrl":"","userId":"02830711679030502652"}},"outputId":"1c53e240-a9ef-4375-bfc4-9fd95b3a9471"},"source":["# Save the generated images.\n","os.makedirs('output', exist_ok=True)\n","for i in range(1000):\n","    torchvision.utils.save_image(imgs_sample[i], f'output/{i+1}.jpg')\n","  \n","# Compress the images.\n","%cd output\n","!tar -zcf ../images.tgz *.jpg\n","%cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/output\n","/content\n"],"name":"stdout"}]}]}